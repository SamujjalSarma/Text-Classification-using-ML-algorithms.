{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Samujjal Seal Sarma            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "1. Importing packages.\n",
    "2. Scraping data from wikipedia various topics as mentioned below using Pythons beautiful soup and create a data frame of various documents.\n",
    "    - Cricket\n",
    "    - Movie\n",
    "    - Technology\n",
    "    - Health\n",
    "3. Cleaning the data by removing stop words and special characters using regex and then lemmetizing the words.\n",
    "4. Creating a document token matrix.\n",
    "5. Using ML algorithms like Logistic regression and random forest for test classification and prediction. Got a prediction of approx 99% through radom forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\samuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\samuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation,PCA\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Getting rid of unnecessary warnings\n",
    "import warnings; \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to scrape data from wikipedia\n",
    "    1. Create URL links for each category: \n",
    "        - Cricket\n",
    "        - Movie\n",
    "        - Technology\n",
    "        - Health\n",
    "    2. Build a Dataframe constituting 500 paragraphs for each of the above categories\n",
    "        - In dataframe, create 2 columns: Paragraph and category\n",
    "        - Build dataframe of size 2000 (500 paragraphs for each category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created dataframe of size:  2000\n",
      "Listing Top few rows: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cricket is a bat-and-ball game played between ...</td>\n",
       "      <td>Cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are various formats ranging from Twenty2...</td>\n",
       "      <td>Cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Historically, cricket's origins are uncertain ...</td>\n",
       "      <td>Cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cricket is one of many games in the \"club ball...</td>\n",
       "      <td>Cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is generally believed that cricket originat...</td>\n",
       "      <td>Cricket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Paragraph Category\n",
       "0  Cricket is a bat-and-ball game played between ...  Cricket\n",
       "1  There are various formats ranging from Twenty2...  Cricket\n",
       "2  Historically, cricket's origins are uncertain ...  Cricket\n",
       "3  Cricket is one of many games in the \"club ball...  Cricket\n",
       "4  It is generally believed that cricket originat...  Cricket"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cricket URLs\n",
    "cric_url = 'https://en.wikipedia.org/wiki/Cricket'\n",
    "t20_url = 'https://en.wikipedia.org/wiki/Twenty20'\n",
    "test_url = 'https://en.wikipedia.org/wiki/Test_cricket'\n",
    "wc_url = 'https://en.wikipedia.org/wiki/Cricket_World_Cup'\n",
    "ipl_url = 'https://en.wikipedia.org/wiki/Indian_Premier_League'\n",
    "t20wc_url = 'https://en.wikipedia.org/wiki/ICC_T20_World_Cup'\n",
    "cricindia_url = 'https://en.wikipedia.org/wiki/Cricket_in_India'\n",
    "indiacricteam_url = 'https://en.wikipedia.org/wiki/India_national_cricket_team'\n",
    "histworldcup_url = 'https://en.wikipedia.org/wiki/History_of_the_ICC_Cricket_World_Cup'\n",
    "champs_url = 'https://en.wikipedia.org/wiki/Champions_League_Twenty20'\n",
    "engcric_url = 'https://en.wikipedia.org/wiki/Cricket#English_cricket_in_the_18th_and_19th_centuries'\n",
    "ltdoverscric_url = 'https://en.wikipedia.org/wiki/Limited_overs_cricket'\n",
    "\n",
    "cricket_urls = [cric_url,  t20_url, test_url, wc_url, ipl_url, t20wc_url, cricindia_url, indiacricteam_url, champs_url,\n",
    "               engcric_url,ltdoverscric_url]\n",
    "#Film URLs\n",
    "film_url = 'https://en.wikipedia.org/wiki/Film'\n",
    "bolly_url = 'https://en.wikipedia.org/wiki/Bollywood'\n",
    "cineindia_url = 'https://en.wikipedia.org/wiki/Cinema_of_India'\n",
    "holly_url = 'https://en.wikipedia.org/wiki/Hollywood'\n",
    "cineus_url = 'https://en.wikipedia.org/wiki/Cinema_of_the_United_States'\n",
    "filmhist_url = 'https://en.wikipedia.org/wiki/History_of_film'\n",
    "\n",
    "film_urls = [film_url, bolly_url, cineindia_url, holly_url, cineus_url, filmhist_url]\n",
    "\n",
    "#Tech URLs\n",
    "tech_url = 'https://en.wikipedia.org/wiki/Technology'\n",
    "techroad_url = 'https://en.wikipedia.org/wiki/Technology_roadmap'\n",
    "techsociety_url = 'https://en.wikipedia.org/wiki/Technology_and_society'\n",
    "techushist_url = 'https://en.wikipedia.org/wiki/Technological_and_industrial_history_of_the_United_States'\n",
    "techanalysis_url = 'https://en.wikipedia.org/wiki/Technical_analysis'\n",
    "techwar_url = 'https://en.wikipedia.org/wiki/Technology_during_World_War_I'\n",
    "techconv_url = 'https://en.wikipedia.org/wiki/Technological_convergence'\n",
    "techit_url = 'https://en.wikipedia.org/wiki/Information_technology'\n",
    "techcomp_url = 'https://en.wikipedia.org/wiki/Computer'\n",
    "techcompport_url='https://en.wikipedia.org/wiki/Portable_computer'\n",
    "\n",
    "tech_urls = [tech_url, techroad_url, techsociety_url, techushist_url, techanalysis_url, techwar_url, techconv_url,\n",
    "            techit_url, techcomp_url, techcompport_url]\n",
    "\n",
    "\n",
    "#Health URLs\n",
    "health_url = 'https://en.wikipedia.org/wiki/Health'\n",
    "healthus_url = 'https://en.wikipedia.org/wiki/Health_care_in_the_United_States'\n",
    "healthins_url = 'https://en.wikipedia.org/wiki/Health_insurance'\n",
    "healthcare_url = 'https://en.wikipedia.org/wiki/Health_care'\n",
    "healthcan_url = 'https://en.wikipedia.org/wiki/Healthcare_in_Canada'\n",
    "healthport_url = 'https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act'\n",
    "healthtob_url = 'https://en.wikipedia.org/wiki/Health_effects_of_tobacco'\n",
    "\n",
    "health_urls = [health_url, healthus_url, healthins_url, healthcare_url, healthcan_url,healthport_url, healthtob_url ]\n",
    "\n",
    "all_categ_urls = [cricket_urls, film_urls, tech_urls, health_urls ]\n",
    "topics = [\"Cricket\",\"Films\",\"Tech\",\"Health\"]\n",
    "iTopic = 0\n",
    "df = pd.DataFrame(columns = ['Paragraph','Category'])\n",
    "\n",
    "#Preparing dataframe out of urls paragraph data\n",
    "topic_limit = 500\n",
    "i = 0\n",
    "for categ_url in all_categ_urls:\n",
    "    topic_ctr = 0\n",
    "    limit_reached = 0\n",
    "    topic = topics[iTopic]\n",
    "    iTopic += 1\n",
    "    for url in categ_url:\n",
    "            if (limit_reached == 1):\n",
    "                break\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            table = soup.find_all('p',attrs={'class': None})\n",
    "            for x in table:\n",
    "                if(len(str(x.get_text().strip())) > 0):\n",
    "                    topic_ctr = topic_ctr + 1\n",
    "                    df.loc[i] = [str(x.get_text()),topic]\n",
    "                    i = i + 1\n",
    "                    if (topic_ctr >= topic_limit ):\n",
    "                        limit_reached = 1\n",
    "                        break\n",
    "print(\"Succesfully created dataframe of size: \",len(df))\n",
    "print(\"Listing Top few rows: \")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps performed in below code:\n",
    "1. Cleaning:\n",
    "    - Special characters (e.g. !,@, #)\n",
    "    - Single characters (e.g. a, i)\n",
    "    - Numbers\n",
    "    - Multiple spaces and prefixes\n",
    "2. Lemmatization\n",
    "3. Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed cleaning process successfully for dataframe of size:  2000\n",
      "Listing top few rows after cleaning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as the stalemate develop on the ground , with...</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>numerous canal company have also be charter ;...</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>healthcare in canada be deliver through thirt...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the importance of stone tool , circa 2.5 mill...</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an example of a security that have an apparen...</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Paragraph Category\n",
       "0   as the stalemate develop on the ground , with...     Tech\n",
       "1   numerous canal company have also be charter ;...     Tech\n",
       "2   healthcare in canada be deliver through thirt...   Health\n",
       "3   the importance of stone tool , circa 2.5 mill...     Tech\n",
       "4   an example of a security that have an apparen...     Tech"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining function for cleaning text\n",
    "def cleanText(text):\n",
    "    '''text = re.sub('[^ a-zA-Z]','',text)\n",
    "    text = re.sub(r' +', ' ', text)'''\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(text))\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)     \n",
    "    # Remove numbers characters from the start\n",
    "    document = re.sub(r'[0-9]', ' ', document)\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    return text\n",
    "\n",
    "#Defining function for lemmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatizeSentences(data):\n",
    "    count = 0    \n",
    "    for text in data[\"Paragraph\"]:\n",
    "        # clean the text\n",
    "        text = cleanText(text)\n",
    "        sentence = \"\"\n",
    "        for i, j in pos_tag(word_tokenize(text)):\n",
    "            word = ''\n",
    "            if j[0].lower() in ['n', 'v', 'r']:\n",
    "                word = wnl.lemmatize(i, j[0].lower())\n",
    "            elif j[0].lower() is 'j':\n",
    "                word = wnl.lemmatize(i, 'a')\n",
    "            else:\n",
    "                word = wnl.lemmatize(i)\n",
    "            sentence = sentence + \" \" + word.lower()\n",
    "        data[\"Paragraph\"].iloc[count] = sentence\n",
    "        count = count + 1;\n",
    "    return data\n",
    "df = lemmatizeSentences(df)\n",
    "#Shuffling the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(\"Completed cleaning process successfully for dataframe of size: \", len(df))\n",
    "print(\"Listing top few rows after cleaning:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps performed in below code:\n",
    "1. DTM creation:\n",
    "    - Each column of dtm will contain number of times each word appears in each category    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>australia</th>\n",
       "      <th>begin</th>\n",
       "      <th>bollywood</th>\n",
       "      <th>care</th>\n",
       "      <th>century</th>\n",
       "      <th>change</th>\n",
       "      <th>cinema</th>\n",
       "      <th>company</th>\n",
       "      <th>computer</th>\n",
       "      <th>cost</th>\n",
       "      <th>...</th>\n",
       "      <th>term</th>\n",
       "      <th>test</th>\n",
       "      <th>time</th>\n",
       "      <th>united</th>\n",
       "      <th>use</th>\n",
       "      <th>war</th>\n",
       "      <th>win</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   australia  begin  bollywood  care  century  change  cinema  company  \\\n",
       "0          0      0          0     0        0       0       0        0   \n",
       "1          0      1          0     0        0       0       0        1   \n",
       "2          0      0          0     1        0       0       0        0   \n",
       "3          0      0          0     0        0       0       0        0   \n",
       "4          0      0          0     0        0       0       0        0   \n",
       "\n",
       "   computer  cost  ...   term  test  time  united  use  war  win  work  world  \\\n",
       "0         0     0  ...      0     0     0       0    1    0    0     0      0   \n",
       "1         0     0  ...      0     0     0       0    0    1    0     0      0   \n",
       "2         0     0  ...      0     0     0       0    0    0    0     0      0   \n",
       "3         0     0  ...      0     0     0       0    0    0    0     0      0   \n",
       "4         0     0  ...      0     0     3       0    0    0    0     0      0   \n",
       "\n",
       "   year  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating dtm\n",
    "vectorizer = CountVectorizer(min_df = 0.05 , max_df= 0.95, stop_words=\"english\")\n",
    "dtm = pd.DataFrame(vectorizer.fit_transform(df['Paragraph']).toarray(),columns=vectorizer.get_feature_names())\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps performed in below code:\n",
    "1. Logistic Regression Algorithm:\n",
    "    - DTM from above code is split into training data (70%) and test data (30%)\n",
    "    - Logistic regression algo is run\n",
    "    - Using test data, model is trained again, re-run the algo to find change in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8983333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(dtm,df[\"Category\"],test_size=0.30)\n",
    "logisticModel = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "logisticModel.fit(X_train,y_train)\n",
    "logisticModel.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logisticModel,dtm, df['Category'].astype('str'),cv=4).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps performed in below code:\n",
    "1. Random Forest regression algorithm:\n",
    "    - DTM from above code is split into training data (70%) and test data (30%)\n",
    "    - Random Forest regression algo is executed\n",
    "    - Using test data, model is trained again, re-run the algo to find change in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dtm columns to be used as training and test\n",
    "columnList = list(dtm.columns.values)\n",
    "trainingColumns = int(len(columnList)-1)\n",
    "X = columnList[:trainingColumns]\n",
    "y = columnList[:len(columnList)-trainingColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9989549614861805"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(dtm,dtm[y],test_size=0.30)\n",
    "randomModel = RandomForestRegressor(n_estimators=100, random_state=42)  \n",
    "randomModel.fit(X_train1, y_train1) \n",
    "randomModel.score(X_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
